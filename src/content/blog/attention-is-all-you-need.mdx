---
draft: false
title: "Attention is All You Need"
snippet: "This post is an interpretation of the paper \"Attention is All You Need\". The purpose is to extend on The Annotated Transformer by further reordering and simplification of the"
publishDate: "2024-01-01 00:00"
image: {
  src: "https://images.unsplash.com/photo-1542393545-10f5cde2c810?&fit=crop&w=430&h=240",
  alt: "typography"
}
category: "Data Science"
author: "Max van Dijck"
tags: [AI, datascience]
---

This post is an interpretation of the paper "Attention is All You Need". The purpose is to extend on [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
by further reordering and simplification of the paper. The motivation behind this is to increase clarity by starting with our input tensors and working through the model layer by layer rather
than starting at a point already deep in the weeds (at the encoder). Rather, we will build up to that point by starting at an input sentence, tokenizing and then creating the $Q\:K\:V$ or query, key and value matrices. Since this paper is a common starting point for those learning about the transformer architecture we try to make this post as approachable as possible and easy to understand without large amounts of prior context or knowledge.

## Tokenisation, Embedding and Linear Transformation

To get started at the very input of the transformer, let's assume a simple sentence and create an example tokenization for the sentence. Normally we would use a [tokenizer](https://platform.openai.com/tokenizer) but we will skip this for the sake of simplicity, the short explanation is that a tokenizer groups relevant neighboring characters together from a given string and converts them into a numerical representation so that they can be used as an input to a neural network. Here is our tensor, we include the batch dimension as to best reflect what happens in a built out example:
```python
sentence = ["our", "input", "sentence"]
input_tokens = torch.tensor([[0, 1, 2]]) # Batch, Sequence
```

Now we can create the embedding layer of our network. An embedding layer stores a mapping from a vocabulary of tokens to a n-dimensional vector space. In our case we use a vocabulary size of three and an embedding size of three since we are only training on this simple example. However, a typical large language model (LLM) would have a vocabulary size in the tens of thousands and embedding size in the thousands. Now if we pass our tensor to the embedding layer we will get an output of shape: Batch, Sequence, Embedding. Viewing the tensor after the embedding we can see each token in our sequence has been mapped to vector space, each being represented by an embedding of three values.

```python
vocabulary_size = 3
embedding_size = 3
embedding = torch.nn.Embedding(vocabulary_size, embedding_size)
embedded_sentence = embedding(input_tokens) # Batch, Sequence, Embedding
```
```
embedded_sentence
> tensor([[[ 0.6380, -1.4403,  0.9792],   # our
           [-1.5645,  0.0593,  0.4939],   # input
           [-0.1139, -1.0340, -0.0034]]], # sentence
            grad_fn=<EmbeddingBackward0>)
```

Given our embedded vector, we can now create our $Q\:K\:V$ matrices used in the attention mechanism mentioned in the paper. We do this by taking our embedded sentence and feeding it into three seperate fully connected layers to obtain the query, key and value matrices

```python
hidden_size = 64  
linear_transform_Q = torch.nn.Linear(embedding_size, hidden_size)
linear_transform_K = torch.nn.Linear(embedding_size, hidden_size)
linear_transform_V = torch.nn.Linear(embedding_size, hidden_size)

Q = linear_transform_Q(embedded_sentence) # batch, sequence, embedding_size
K = linear_transform_K(embedded_sentence) # """
V = linear_transform_V(embedded_sentence) # """
```

## Scaled Dot-product Attention

With our $Q\:K\:V$ matrices we will now perform the scaled dot-product attention introduced in the paper. We take the query matrix and multiply it by the transposed key matrix. Then we divide the resulting matrix by the square root length of the transformed embeddings $\sqrt{d_{k}}$, which is equal to the square-root of the linear transform's hidden size, in our case 64 -> 8. After this division step the output is optionally masked and put through a softmax function. Then dropout is applied and finally multiplied by the value matrix. The equation shown in the paper is the following (which excludes detailing the mask and dropout which we will get into, along with the purpose of the query, key and value matrices and the intuition behind them.

$Attention(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V $

And implemented as Pytorch code

```python
def attention(
      query: torch.Tensor, 
      key: torch.Tensor, 
      value: torch.Tensor, 
      mask: torch.Tensor | None = None, 
      dropout: torch.nn.Module | None = None
    ):
    query_embedding_len = query.size(-1) # same as embedding hidden size (64 in example)
    key = key.transpose(-2, -1) # batch, embedding_size, sequence
    attention_scores = query @ key / math.sqrt(query_embedding_len) # batch, sequence_length, attention_scores
    if mask:
        attention_scores = attention_scores.masked_fill(mask == 0, -1e9)
    attention_scores = attention_scores.softmax(dim=-1)
    if dropout:
        attention_scores = dropout(attention_scores)
    return attention_scores @ value, attention_scores
```

### Query, Key, Value

Now, we're familiar with a basic dictionary/hashmap which works to map from an exact key to a specific value

```python
d = {
   "fun": 10,
   "exciting": 9,
   "boring": -3,
 }
d["fun"] # 10
```

However, we want to do a lookup based on the meaning of the word and return a vector containing how likely it is that each key in our lookup table is likely related to our query, or rather how much *attention* we want to pay to each value based on the query. So for example, let's take the word "positive" and assume that we have a 45% match for both "fun" and "exciting" and only a 10% match for "boring".

```python
query = "rollercoaster"
0.45 * d["fun"] + 0.45 * d["exciting"] + 0.1 * d["boring"] # 5.65
```

In this example, I've created the attention scores based upon how relevant I interpret each key to the query "rollercoaster". So how does a model internally arrive at these attention scores? Now if we think back to the previous section we will be reminded that our query and key matrices have the dimensions Sequence and Embedding (plus batch as the first dimension, which we don't really need to worry about). The embedding is the crucial part here and having an understanding of how dot products work in relation to the similarity between two vectors helps. The key ideas are as follows
- If two vectors are in the same direction their dot product is at it's largest $n > 0$
- If two vectors are in the opposite direction their dot product is at it's smallest $n < 0$
- If two vectors are perpindicular their dot product will be zero $n = 0$

So when we have two *word embeddings*, our query vector's word embedding and our key vector's word embedding, and we take the dot product we get a measure of how relevant the key is in relation to the query based on how similar the two are in vector space. We've created a [desmos visualization](https://www.desmos.com/calculator/b7xvrosoyp) with two vectors and their resulting dot product to demonstrate this behaviour. This similarity is called the attention score. What's happened is that our query embedding is a vector that when mapped into the vector space of the key, is surrounded by tokens that are most relevant to pay attention to. If we change our keys into vectors where our hash-function is now similar to $query \cdot value$ we now get something that looks a bit like this:

```python
query = [8, -5]
d = {
  [5, -3]: 10, # fun 
  [6, -1]: 9,  # exciting
  [-4, 8]: 3, # boring
}
attention_scores = [8*5 + -5*-3, 8*6 + -5*-1, 8*-4 + -5*8] # [55, 53, -72]
```

As you can see if our query embedding is close within the key vector space to the "fun" and "exciting" keys it results in a high attention to these values and opposite to the "boring" key resulting in a low attention score. 

![Vector space](../../assets/vector-space.png)

Translating our dictionary example in our pytorch code this is what is happening in the following

```python
key = key.transpose(-2, -1)
attention_scores = query @ key
```

Following this, the attention scores are then divided by the square-root of the embedding length and put through a softmax function. As the embedding length grows in size the resultant dot products can get quite large in magnitude which would make the output values from the softmax extremely close to 0 and 1 which may cause vanishing gradients. This problem is why we scale the attention scores (henced *scaled* dot product attention). And now we multiply our attention scores with the values to create a matrix where each element of the sequence contains a new vector embedding that is a weighted combination of the orignal value matrix's embedding for each work by the attention scores for that particular token.

```diff
+ query_embedding_len = query.size(-1)
  key = key.transpose(-2, -1)
- attention_scores = query @ key
+ attention_scores = query @ key / math.sqrt(query_embedding_len)
+ attention_scores = attention_scores.softmax(dim=-1)
+ return attention_scores @ value, attention_scores
```

Now we did skip the masking and dropout layer to focus on the explanation of the $Q\:K\:V$ matrices. Dropout is as expected used as a regularization method to prevent the transformer for overfitting. The masking is used in the decoder stack to prevent current predictions from referencing tokens in the future which could result in incorrect sequence generation. We will dive deeper into this when we address the decoder stack.

## Multi-Head Attention

Multi-Head Attention consists of combining our linear transform step and the scaled dot-product attention step into a single function, this becomes a single head, and applying this function multiple times on the input embedded vector. We take the results from each head and simply concatentate them together, then pass the result through a final linear layer as shown in the following figure. The idea is that the multiple heads create a number of independently modelled word associations with different vector spaces for the $Q\:K\:V$ matrices, this could result in individual heads learning particular relationships such as pronouns to nouns (eg. Her hat) or noun to verb (eg. Door closes). The concatenated results are then put through another linear layer to finalise our multi-head attention

![Multi-Head Attention](../../assets/multihead-attention.png)

The following python is multihead attentition with 3 heads, and no mask or dropout using the attention function we defined earlier. In this example we use a loop to iterate over each individual head sequentially for simplicity, however in practice we would want to parallelise this.

```python
class Head(torch.nn.Module):
  def __init__(self, input_embedding_size: int, hidden_size: int):
    super().__init__() 
    self.linear_transform_Q = torch.nn.Linear(input_embedding_size, hidden_size)
    self.linear_transform_K = torch.nn.Linear(input_embedding_size, hidden_size)
    self.linear_transform_V = torch.nn.Linear(input_embedding_size, hidden_size)
  def forward(self, x: torch.Tensor) -> torch.Tensor:
    Q = self.linear_transform_Q(x) # batch, sequence, hidden_size
    K = self.linear_transform_K(x) # """
    V = self.linear_transform_V(x) # """
    output, _ = attention(Q, K, V)
    return output

num_heads = 3
input_embedding_size = 3
hidden_size = 64

heads = [Head(input_embedding_size, hidden_size) for _ in range(num_heads)]
fc_out = torch.nn.Linear(hidden_size * num_heads, input_embedding_size * num_heads)

mock_embedded_sentence = torch.randn(1, 3, input_embedding_size * num_heads) # Batch, Sequence, Embedding * Number of Heads
mock_embedded_sentence_split = torch.split(mock_embedded_sentence, num_heads, dim=2)

out = []
for i in range(num_heads):
  out.append(heads[i](mock_embedded_sentence_split[i]))

concat = torch.cat(out, dim=2) # batch, sequence, hidden_size * num_heads
out = fc_out(concat)
```

## Encoder

To finish the encoder mechanism we will add the recurrent connections, layer normalization and linear layer as illustrated in the digram from the paper. (Note: the position embedding will be covered independently) We start by making the `EncoderTransformerBlock` which is what is contained within the main block of the follwoing diagram and then the `Encoder` which contains the six repeated transformer blocks.

![Encoder Mechanism](../../assets/encoder.png)

```python
class EncoderTransformerBlock(torch.nn.Module):
  def __init__(self, input_embedding_size: int, num_heads: int):
    super().__init__()
    self.num_heads = num_heads
    # Multi-Head Attention
    self.heads = [Head(input_embedding_size, hidden_size) for _ in range(num_heads)]
    self.mha_fc_out = torch.nn.Linear(hidden_size * num_heads, input_embedding_size * num_heads)

    # First Normalisation
    self.norm1 = nn.LayerNorm(input_embedding_size * num_heads)

    # Feed Forward
    self.feed_foward = nn.Sequential(
       nn.Linear(input_embedding_size * num_heads, input_embedding_size * num_heads * forward_expansion),
       nn.ReLU(),
       nn.Linear(input_embedding_size * num_heads * forward_expansion, input_embedding_size * num_heads)
    )

    # Second Normalisation
    self.norm2 = nn.LayerNorm(input_embedding_size * num_heads)


  def forward(x):
    # Multi-Head Attention
    split = torch.split(x, self.num_heads, dim=2)

    out = []
    for i in range(self.num_heads):
      out.append(heads[i](split[i]))

    out = torch.cat(out, dim=2) # batch, sequence, hidden_size * num_heads
    out = fc_out(out)

    # First Normalisation
    x = self.norm1(out + x)

    # Feed Forward
    ff = self.feed_forward(x)

    # Second Normalization
    x = self.norm2(ff + x)

    return x
```
    




